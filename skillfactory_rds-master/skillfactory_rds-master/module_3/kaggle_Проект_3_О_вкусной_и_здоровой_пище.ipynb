{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Итоговое задание Соколова Александра**  \n",
    "### по Проекту 3. О вкусной и здоровой пище \n",
    "####  Юнит 3. Введение в машинное обучение (отредактирован 15.06.2020)\n",
    "---\n",
    "\n",
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sokaa.my_module (from versions: none)\n",
      "ERROR: No matching distribution found for sokaa.my_module\n"
     ]
    }
   ],
   "source": [
    "!pip install sokaa.my_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn_pandas import DataFrameMapper\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "from IPython.display import display\n",
    "from importlib import reload\n",
    "# import cellbell\n",
    "#print(os.listdir('../kaggle/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sokaa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7d30614fe82b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msokaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmy_module\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sokaa'"
     ]
    }
   ],
   "source": [
    "import sokaa.my_module as my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:\\\\Users\\\\user\\\\Documents\\\\DataScience\\\\PYTHON\\\\skillfactory_rds-master\\\\skillfactory_rds-master\\\\module_3\\\\kaggle\\\\data\\\\main_task.csv')\n",
    "df_test = pd.read_csv('C:\\\\Users\\\\user\\\\Documents\\\\DataScience\\\\PYTHON\\\\skillfactory_rds-master\\\\skillfactory_rds-master\\\\module_3\\\\kaggle\\\\data\\\\kaggle_task.csv')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "display(df_train.head(2))\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['Sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['Sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "df = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Детальный анализ по переменным\n",
    "---\n",
    "### 1. Restaurant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['code_Restaurant_id'] = df['Restaurant_id'].apply(lambda x: float(x[3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - Restaurant_id:*** Визуальный осмотр показал, что код Restaurant_id очень сильно похож на Ranking в data_train. Надо проверить корреляцию и при необходимости удалить 'code_Restaurant_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cuisine Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в переменной 9283 (23.2%) пропущенных значений \n",
    "# сохраним эту информацию\n",
    "df['NAN_Cuisine Style'] = pd.isna(df['Cuisine Style']).astype('float64') \n",
    "\n",
    "# заполним пропуски значением 'Other'\n",
    "df['Cuisine Style'] = df['Cuisine Style'].fillna(\"['Other']\")\n",
    "\n",
    "# закодируем значения в переменной до их преобразования\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Cuisine Style'])\n",
    "df['code_Cuisine Style'] = le.transform(df['Cuisine Style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проведем обработку значений переменной\n",
    "df['Cuisine Style'] = df['Cuisine Style'].str.findall(r\"'(\\b.*?\\b)'\") \n",
    "\n",
    "temp_list = df['Cuisine Style'].tolist()\n",
    "\n",
    "def list_unrar(list_of_lists):\n",
    "    result=[]\n",
    "    for lst in list_of_lists:\n",
    "      result.extend(lst)\n",
    "    return result\n",
    "\n",
    "temp_counter=Counter(list_unrar(temp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем список достаточно уникальных кухонь и сформируем на его основе новый признак\n",
    "list_of_unique_Cuisine = [x[0] for x in temp_counter.most_common()[-16:]]\n",
    "df['unique_Cuisine_Style'] = df['Cuisine Style'].apply(lambda x: 1 if len(set(x) & set(list_of_unique_Cuisine))>0  else 0).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cuisine in temp_counter:\n",
    "    df[cuisine] = df['Cuisine Style'].apply(lambda x: 1 if cuisine in x else 0 ).astype('float64')\n",
    "\n",
    "# генерируем новый признак кол-во кухонь в ресторане\n",
    "df['count_Cuisine_Style'] = df['Cuisine Style'].apply(lambda x: len(x)).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.four_plot_with_log('count_Cuisine', df[df['Sample'] == 1].count_Cuisine_Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видимо в значениях есть нули, это значит что это не Other, а просто не заполненные. проверим\n",
    "my.describe_without_plots('count_Cuisine_Style', df[df['Sample'] == 1].count_Cuisine_Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - Cuisine Style:*** Много пропусков 9283 (23.2%). Чтобы сохранить эту информацию сформирован новый признак 'NAN_Cuisine Style'. С помощью интелектуального кодера закодировал изначальные значения и ввел новый критерий - 'code_Cuisine Style'. Значения критерия обработаны с помощью регулярных выражений, чтобы составить статистику по типам кухонь. По самым не популярным составлен признак - 'unique_Cuisine_Style'. Кроме этого, после обработки, добавлен критерий количества типов кухонь в ресторане 'count_Cuisine_Style'. В новом признаке выбросов нет. В исходном критерии все типы кухонь были заполнены, [] пустых списков не было."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Price Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в переменной очень много пропусков 13886 (34.7%)\n",
    "# сохраним информацию о пропусках чтобы не потерять\n",
    "df['NaN_Price Range'] = pd.isna(df['Price Range']).astype('float64') \n",
    "\n",
    "# заполним значения в переменной по словарю\n",
    "dic_value_Price = {'$':1,'$$ - $$$':2,'$$$$':3}\n",
    "df['Price_Range']=df['Price Range'].map(lambda x: dic_value_Price.get(x,x))\n",
    "\n",
    "# 18412 ресторанов это более 70% из заполненной информации имеют средний параметр цены\n",
    "# поэтому заполняем пропуски двойкой (2)\n",
    "df['Price_Range'] = df['Price_Range'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим полученный критерий\n",
    "my.describe_with_hist('Price Range', df[df['Sample'] == 1].Price_Range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - Price Range:*** в переменной очень много пропусков 13886 (34.7%). Чтобы не потерять информацию о пропусках создан новый критерий - 'NaN_Price Range'. Также создан новый признак числового кодирования цены - 'Price_Range' (низкая цена - 1, средняя цена -2 , высокая цена - 3).  Пропуски заполнили модой - средней ценой (двойкой). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Number of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в переменной 2543 (6.4%) пропущенных значений \n",
    "# сохраним эту информацию\n",
    "df['NAN_Number of Reviews'] = pd.isna(df['Number of Reviews']).astype('float64')\n",
    "\n",
    "# для удобства изменим название столбца\n",
    "df.rename(columns={'Number of Reviews': 'Number_of_Reviews'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.four_plot_with_log2('Number_of_Reviews', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбросы есть, купол распределения с точками перегиба, необходимо посмотреть на гистограмму по крупнее\n",
    "my.big_hist_log('Number_of_Reviews', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на границы\n",
    "my.borders_of_outliers('Number_of_Reviews', df[df['Sample'] == 1], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбросов не так много, удалим их, предварительно сохранив информацию о них\n",
    "df['outliers_Number_of_Reviews'] = pd.DataFrame(df['Number_of_Reviews']>5252).astype('float64')\n",
    "df.loc[df['Number_of_Reviews']>5252, 'Number_of_Reviews']=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - Number of Reviews*** 2543 (6.4%) пропусков.  \n",
    "Странный вид прологарифмированного распределения, как будто в нем два распределения причем одно логнормальное, а другое распределение Бернули при малом кол-ве кухонь. Второе распределение может быть искуственным снижением или удалением кол-ва отзывов (возможно за \"фейковые\" отзывы или \"накрутки\")  Необходимо попытаться сделать срезы на других переменных (City) или на новых переменных (Population и т.п.), если хватит времени.  \n",
    "Выбросов 10. Удалил. Сохранил информацию о них в отдельной переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в ревью нет пропусков, но 6471 строк со значением [[], []]. По сути это пустые строки сохраним их \n",
    "df['empty_Reviews'] = (df['Reviews']=='[[], []]').astype('float64')\n",
    "\n",
    "# анализ тестовой базы выявил два пропуска, несмотря на то, что pandas.profiling на тренировочной базе пропусков не выявил, заполним их '[[], []]' и закинем в empty_Reviews\n",
    "df['Reviews'] = df['Reviews'].fillna('[[], []]')\n",
    "df['empty_Reviews'] = (df['Reviews']=='[[], []]').astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вытащим дату из ревью и создадим новые критерии\n",
    "df['date_of_Review'] = df['Reviews'].str.findall('\\d+/\\d+/\\d+')\n",
    "df['len_date'] = df['date_of_Review'].apply(lambda x: len(x))\n",
    "\n",
    "# проверим длину дат, на случай если там больше или меньше двух (2)\n",
    "my.describe_without_plots('len_date', df[df['Sample'] == 1].len_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# есть значение 3 надо разобраться что там\n",
    "print(\"кол-во значений Reviews с тремя датами :=\" , len(df[df['len_date']==3]))\n",
    "print(\"значения Reviews с тремя датами :=\")\n",
    "temp_list = df[df['len_date']==3].Reviews.to_list()\n",
    "display(df[df['len_date']==3].Reviews.to_list())\n",
    "display([re.findall('\\d+/\\d+/\\d+', x) for x in temp_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видим что люди указывали даты в отзывах и эти даты попали в обработку\n",
    "# из-за этого возникнут ошибки так как даты не верные и их формат отличается и формата выгрузки\n",
    "# при этом таких строк всего четыре (4), можно было бы их не исправлять а выбросить потому что 17 \n",
    "# год явно приведет к выбросу с которым надо будет разбираться. Выбрасывать жалко, тогда исправим,\n",
    "# тем более, что это достачно просто\n",
    "\n",
    "df['len_date'].date_of_Review = df[df['len_date']==3].date_of_Review.apply(lambda x: x.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# также есть значение 1 надо разобраться что там\n",
    "print(\"кол-во значений Reviews с одной датой :=\" , len(df[df['len_date']==1]))\n",
    "display(df[df['len_date']==1].Reviews[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оказалось, что есть отзывы с одним (1) отзывом и их достаточно много 5680 из (40000-6471) это 17%\n",
    "# сохраним это на всякий случай, чтобы не потерять\n",
    "df['one_Review'] = (df['len_date']==1).astype('float64')\n",
    "\n",
    "# заполним перерыв между отзывами (по отзывам где len = 2) и насколько давно был сделан последний самый свежий отзыв\n",
    "# создадим для этого функции:\n",
    "def time_to_now(row):\n",
    "    if row['date_of_Review'] == []:\n",
    "        return None\n",
    "    return pd.datetime.now() - pd.to_datetime(row['date_of_Review']).max()\n",
    "\n",
    "def time_between_Reviews(row):\n",
    "    if row['date_of_Review'] == []:\n",
    "        return None\n",
    "    return pd.to_datetime(row['date_of_Review']).max() - pd.to_datetime(row['date_of_Review']).min()\n",
    "\n",
    "df['day_to_now'] = df.apply(time_to_now, axis = 1).dt.days\n",
    "df['day_between_Reviews'] = df[df['len_date']==2].apply(time_between_Reviews, axis = 1).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на критерий day_to_now - это насколько давно был сделан последний самый свежий отзыв в днях\n",
    "my.four_plot_with_log2('day_to_now', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбросов достаточно много посмотрим на границы выбросов\n",
    "my.borders_of_outliers('day_to_now', df[df['Sample'] == 1], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# жалко терять 2356 значений \n",
    "# посмотрим на гистограмму крупно\n",
    "my.big_hist_log('day_to_now', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# никаких очевидных аномалий не видно \n",
    "# посмотрим основные статистики\n",
    "my.describe_without_plots('day_to_now', df[df['Sample'] == 1].day_to_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - Reviews:*** Пропусков в тренировочном датасете нет, в тестовом - 2. Но есть 6471 незаполненных строк с отзывами в тренировочном датасете это 16% от датасета. В 5680 (14%) строках есть только один отзыв, хотя в подавляющем большинстве отзывов два.  \n",
    "Созданы новые критерии:   \n",
    "- empty_Reviews - незаполненные отзывы  \n",
    "- date_of_Review - даты из отзывов  \n",
    "- len_date - кол-во дат в отзыве  \n",
    "- day_to_now - насколько давно был сделан последний самый свежий отзыв в днях  \n",
    "- day_between_Reviews - перерыв между отзывами в днях\n",
    "\n",
    "***Резюме - day_to_now из Reviews:*** Удаление по порогу не напрашивается так как компания TripAdvisor работает с 2000 года. Максимум 5896/365 ~ 16,5 лет от 2020 года укладывается в дату начала старта сайта. В выбросы попало 2365 (почти 6%) значений, с учетом резюме по неполным данным в критерии Reviews, я пока принимаю решение не избавлятся от выросов, построить модель, обратить внимание на важность критерия, и при необходимости вернуться к нему для заполнения парсингом или удаления выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь посмотрим на разницу в датах отзывов в днях \n",
    "my.four_plot_with_log2('day_between_Reviews', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.big_hist_log('day_between_Reviews', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.borders_of_outliers('day_between_Reviews', df[df['Sample'] == 1], log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кол-во выбросов 495 (1.2%) - это статистически не значимо, но мы пока сохраняем информацию о выбросе, а потом проверим его важность в модели\n",
    "df['out_day_between_Reviews'] = (df['day_between_Reviews']==0).astype('float64')\n",
    "\n",
    "# и удаляем выбросы\n",
    "df.loc[df['day_between_Reviews']==0, 'day_between_Reviews'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.describe_without_plots('day_between_Reviews', df[df['Sample'] == 1].day_between_Reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - day_between_Reviews из Reviews:*** Заполнен слабо 70%. Удалены выбросы в нуле (492 значения). Создан новый критерий - out_day_between_Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>***Позитивный или негативный оттенок в отзывах***</summary>\n",
    "    Этот пункт пока не проработан, по причине того, что мне кажется эффективнее потратить время на парсинг дополнительной информации, чем на обработку этого момента. Так как я решил пока не использовать парсинг и выжать максимум из модели, проработаю если хватит времени\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ID_TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['code_ID_TA'] = df['ID_TA'].apply(lambda x: float(x[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. URL_TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['code_after_g_URL_TA'] = df['URL_TA'].str.split('-').apply(lambda x: x[1][1:]).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_City_dummies = pd.get_dummies(df['City'], dummy_na=False).astype('float64')\n",
    "df = pd.concat([df,df_City_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df['City'])\n",
    "df['code_City'] = le.transform(df['City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Capital_City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_Of_NotCapitalCity = ['Barcelona', 'Milan', 'Hamburg', 'Munich', \n",
    "                          'Lyon', 'Zurich', 'Oporto', 'Geneva', 'Krakow']\n",
    "df['Capital_City'] = df['City'].apply(lambda x: 0.0 if x in list_Of_NotCapitalCity else 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Сountries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_Сountries = {'London' : 'England', 'Paris' : 'France', 'Madrid' : 'Spain', \n",
    "                  'Barcelona' : 'Spain', 'Berlin' : 'Germany', 'Milan' : 'Italy', \n",
    "                  'Rome' : 'Italy', 'Prague' : 'Czech_c', 'Lisbon' : 'Portugal', \n",
    "                  'Vienna' : 'Austria', 'Amsterdam' : 'Holland', \n",
    "                  'Brussels' : 'Belgium', 'Hamburg' : 'Germany', 'Munich' : 'Germany', \n",
    "                  'Lyon' : 'France', 'Stockholm' : 'Sweden', 'Budapest' : 'Romania', \n",
    "                  'Warsaw' : 'Poland', 'Dublin' : 'Ireland', 'Copenhagen' : 'Denmark', \n",
    "                  'Athens' : 'Greece', 'Edinburgh' : 'Scotland', 'Zurich' : 'Switzerland', \n",
    "                  'Oporto' : 'Portugal', 'Geneva' : 'Switzerland', 'Krakow' : 'Poland', \n",
    "                  'Oslo' : 'Norway', 'Helsinki' : 'Finland', 'Bratislava' : 'Slovakia', \n",
    "                  'Luxembourg' : 'Luxembourg_c', 'Ljubljana' : 'Slovenia'}\n",
    "df['Сountry'] = df.apply(lambda row: dict_Сountries[row['City']], axis = 1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Сountry'])\n",
    "df['code_Сountry'] = le.transform(df['Сountry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4. Сity_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_Сity_population= {'London' : 8908, 'Paris' : 2206, 'Madrid' : 3223, 'Barcelona' : 1620, \n",
    "                        'Berlin' : 6010, 'Milan' : 1366, 'Rome' : 2872, 'Prague' : 1308, \n",
    "                        'Lisbon' : 506, 'Vienna' : 1888, 'Amsterdam' : 860, 'Brussels' : 179, \n",
    "                        'Hamburg' : 1841, 'Munich' : 1457, 'Lyon' : 506, 'Stockholm' : 961, \n",
    "                        'Budapest' : 1752, 'Warsaw' : 1764, 'Dublin' : 553, \n",
    "                        'Copenhagen' : 616, 'Athens' : 665, 'Edinburgh' : 513, \n",
    "                        'Zurich' : 415, 'Oporto' : 240, 'Geneva' : 201, 'Krakow' : 769, \n",
    "                        'Oslo' : 681, 'Helsinki' : 643, 'Bratislava' : 426, \n",
    "                        'Luxembourg' : 119, 'Ljubljana' : 284}\n",
    "df['Сity_population'] = df.apply(lambda row: dict_Сity_population[row['City']], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Резюме - City:*** Без пропусков, категориальный признак. Сгенерированы новые признаки города по типу dummies, также создан новый критерий code_City с интелектуальной кодировкой LabelEncoder из библиотеки sklearn  \n",
    "Добавлены новые критерии:  \n",
    "- Capital_City - столица  \n",
    "- code_Сountry - код страны с помощью LabelEncoder\n",
    "- Сity_population - население города (тыс. чел.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.four_plot_with_log2('Ranking', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.big_hist('Ranking', df[df['Sample'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?\n",
    "plt.rcParams['figure.figsize'] = (14,4)\n",
    "df_train['City'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на топ 10 городов\n",
    "for x in (df_train['City'].value_counts())[0:10].index:\n",
    "    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получается, что Ranking имеет нормальное распределение, \n",
    "# просто в больших городах больше ресторанов, из-за мы этого имеем смещение\n",
    "# необходимо отнормировать критерий Ranking по городам City\n",
    "mean_Ranking_on_City = df.groupby(['City'])['Ranking'].mean()\n",
    "count_Restorant_in_City = df['City'].value_counts(ascending=False)\n",
    "df['mean_Ranking_on_City'] = df['City'].apply(lambda x: mean_Ranking_on_City[x])\n",
    "df['count_Restorant_in_City'] = df['City'].apply(lambda x: count_Restorant_in_City[x])\n",
    "df['norm_Ranking_on_Rest_in_City'] = (df['Ranking'] - df['mean_Ranking_on_City']) / df['count_Restorant_in_City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим что получилось на топ 10 городов\n",
    "for x in (df['City'].value_counts())[0:10].index:\n",
    "    df['norm_Ranking_on_Rest_in_City'][df['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_Ranking_on_City = df.groupby(['City'])['Ranking'].max()\n",
    "df['max_Ranking_on_City'] = df['City'].apply(lambda x: max_Ranking_on_City[x])\n",
    "df['norm_Ranking_on_maxRank_in_City'] = (df['Ranking'] - df['mean_Ranking_on_City']) / df['max_Ranking_on_City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (df['City'].value_counts())[0:10].index:\n",
    "    df['norm_Ranking_on_maxRank_in_City'][df['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# критерий Ranking по населению в городах Population_Сity\n",
    "mean_Ranking_on_City = df.groupby(['City'])['Ranking'].mean()\n",
    "df['mean_Ranking_on_City'] = df['City'].apply(lambda x: mean_Ranking_on_City[x])\n",
    "df['norm_Ranking_on_Popul_in_City'] = (df['Ranking'] - df['mean_Ranking_on_City']) / df['Сity_population']\n",
    "\n",
    "for x in (df['City'].value_counts())[0:10].index:\n",
    "    df['norm_Ranking_on_Popul_in_City'][df['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_Population_on_Rest'] = df['Сity_population']/df['count_Restorant_in_City']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построчная верификация первых двух строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление нечисловых критериев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Restaurant_id', 'City', 'Cuisine Style', 'Price Range', 'Reviews', 'URL_TA', 'ID_TA', 'date_of_Review', 'len_date', 'Сountry', 'Сity_population', 'mean_Ranking_on_City', 'count_Restorant_in_City', 'max_Ranking_on_City', ], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартизируем и заполняем нулями пропуски по всем переменным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для стандартизации\n",
    "def StandardScaler_column(d_col):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[[d_col]])\n",
    "    return scaler.transform(df[[d_col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизируем все столбцы кроме целевой и Sample\n",
    "for i  in list(df.columns):\n",
    "    if i not in ['Rating','Sample']:\n",
    "        df[i] = StandardScaler_column(i)\n",
    "        if len(df[df[i].isna()]) < len(df):\n",
    "            df[i] = df[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем заполнение\n",
    "display(df.describe().head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиваем датасет на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.query('Sample == 1').drop(['Sample'], axis=1)\n",
    "test_data = df.query('Sample == 0').drop(['Sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучаем модель, генерируем результат и сравниваем с тестом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_round(d_num):\n",
    "    return int(d_num + (0.5 if d_num > 0 else -0.5))\n",
    "\n",
    "def my_round(d_pred):\n",
    "    result = classic_round(d_pred*2)/2\n",
    "    if result <=5:\n",
    "        return result\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "my_vec_round = np.vectorize(my_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_vec_round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print('MAE:', MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\n",
    "plt.rcParams['figure.figsize'] = (12,10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверяем корреляцию важных переменных и применяем метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.loc[df['Sample'] == 1, list(feat_importances.nlargest(15).index[0:15])]\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "ax = sns.heatmap(df_temp.corr(), annot=True, fmt='.2g')\n",
    "i, k = ax.get_ylim()\n",
    "ax.set_ylim(i+0.5, k-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_temp = list(feat_importances.nlargest(15).index[[9,10]])\n",
    "display(df_temp[list_temp].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Резюме по критерию code_Restaurant_id. Удаляем так как была гипотеза о корреляции с Ranking\n",
    "df.drop(['code_Restaurant_id'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_temp = list(feat_importances.nlargest(15).index[[0,1,6,10]])\n",
    "df_temp[list_temp].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод главных компонент, PCA\n",
    "C = np.array([\n",
    "    [       1, 0.999832, 0.800703, 0.574781],\n",
    "    [0.999832,        1, 0.796851, 0.570877],\n",
    "    [0.799654, 0.796851,        1, 0.448070],\n",
    "    [0.574781, 0.570877, 0.448070,        1]]) \n",
    "eig_num, eig_v = np.linalg.eig(C)\n",
    "print(f\"вектор главных компонент := {eig_v[:,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_Ranking_PCA'] = eig_v[:,0][0]*df['norm_Ranking_on_maxRank_in_City'] + eig_v[:,0][1]*df['norm_Ranking_on_Rest_in_City'] + eig_v[:,0][2]*df['norm_Ranking_on_Popul_in_City']+eig_v[:,0][3]*df['Ranking']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим критерии используя полиномиальный принцип"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ranking_on_square'] = df['Ranking']* df['Ranking']\n",
    "df['doble_Ranking'] = df['Ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.query('Sample == 1').drop(['Sample'], axis=1)\n",
    "test_data = df.query('Sample == 0').drop(['Sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = my_vec_round(y_pred)\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print('MAE:', MAE)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование оптимального набора критериев "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ofAllColumnsSortImportant = list(feat_importances.nlargest(len(train_data.columns)-1).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list2 = ['Australian', 'one_Review', 'outliers_Number_of_Reviews', 'norm_Ranking_on_Popul_in_City', 'Korean', 'Japanese', 'Turkish', 'Malaysian', 'Indonesian', 'Hawaiian', 'code_Cuisine Style', 'norm_Population_on_Rest', 'Amsterdam', 'Hamburg', 'doble_Ranking', 'Warsaw', 'Persian', 'Soups', 'Mexican', 'Bangladeshi', 'Yunnan', 'American', 'empty_Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in drop_list2:\n",
    "    list_ofAllColumnsSortImportant.remove(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(drop_list2) & set(list_ofAllColumnsSortImportant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# блок тестирования оптимального набора\n",
    "min_MAE = round(MAE,3)\n",
    "print(f\"min_MAE = {min_MAE}\")\n",
    "remove_list = []\n",
    "log = []\n",
    "delta =0.004\n",
    "for i in range(0,len(list_ofAllColumnsSortImportant),1):\n",
    "    col = list_ofAllColumnsSortImportant[i]\n",
    "    print(f\"{i}.{col}\")\n",
    "    ###\n",
    "    train_data = df.query('Sample == 1').drop(['Sample']+drop_list2, axis=1)\n",
    "    test_data = df.query('Sample == 0').drop(['Sample']+drop_list2, axis=1)\n",
    "\n",
    "    y = train_data.Rating.values            # наш таргет\n",
    "    X = train_data.drop(['Rating']+[col], axis=1)\n",
    "\n",
    "    # Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "    # выделим 20% данных на валидацию (параметр test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    print(test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred = my_vec_round(y_pred)\n",
    "    temp_MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    ###\n",
    "    print(temp_MAE)\n",
    "    log.append([col, temp_MAE])\n",
    "    if round(temp_MAE,3) <= min_MAE-delta:\n",
    "        remove_list.append(col)\n",
    "        print(f\"удаляем:= {col}\")\n",
    "    else:\n",
    "        print(f\"не удаляем:= {col}\")\n",
    "print(f\"i={i}\")\n",
    "print(f\"remove_list: {remove_list}\")\n",
    "print(f\"log_list: {log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = ['norm_Ranking_on_maxRank_in_City', 0.166125], ['norm_Ranking_on_Rest_in_City', 0.1638125], ['Number_of_Reviews', 0.2305625], ['Rome', 0.165], ['NAN_Number of Reviews', 0.1653125], ['code_ID_TA', 0.1691875], ['norm_Ranking_PCA', 0.16375], ['day_to_now', 0.1669375], ['day_between_Reviews', 0.1661875], ['Ranking_on_square', 0.164625], ['code_City', 0.166375], ['Ranking', 0.1631875], ['code_Сountry', 0.164375], ['code_after_g_URL_TA', 0.16525], ['count_Cuisine_Style', 0.1638125], ['Madrid', 0.163875], ['NaN_Price Range', 0.1666875], ['Price_Range', 0.1640625], ['Edinburgh', 0.1660625], ['Italian', 0.164375], ['Indian', 0.1646875], ['Mediterranean', 0.165], ['Pizza', 0.1655625], ['European', 0.1645], ['Spanish', 0.1643125], ['Vegetarian Friendly', 0.1645625], ['French', 0.1648125], ['Fusion', 0.1635625], ['International', 0.1655625], ['Healthy', 0.1640625], ['Cafe', 0.1655], ['Capital_City', 0.16425], ['Vegan Options', 0.165125], ['Oporto', 0.1645], ['Stockholm', 0.164875], ['Other', 0.1655], ['Fast Food', 0.16475], ['Pub', 0.1651875], ['NAN_Cuisine Style', 0.1651875], ['Chinese', 0.1655], ['Bar', 0.1649375], ['Asian', 0.1649375], ['Milan', 0.1645625], ['Bratislava', 0.165625], ['Portuguese', 0.163875], ['Seafood', 0.1650625], ['Delicatessen', 0.163875], ['Gluten Free Options', 0.16425], ['Wine Bar', 0.164375], ['Dutch', 0.1656875], ['Gastropub', 0.16575], ['German', 0.164625], ['Sushi', 0.1641875], ['Greek', 0.1668125], ['Grill', 0.16475], ['Thai', 0.164625], ['Krakow', 0.1644375], ['Lyon', 0.1644375], ['Central European', 0.1641875], ['Prague', 0.16475], ['Halal', 0.163875], ['British', 0.1656875], ['Czech', 0.16475], ['Budapest', 0.1650625], ['Contemporary', 0.1645], ['Middle Eastern', 0.1654375], ['Barcelona', 0.164375], ['Helsinki', 0.1658125], ['Polish', 0.1651875], ['out_day_between_Reviews', 0.163], ['Munich', 0.1654375], ['Danish', 0.1656875], ['Athens', 0.16425], ['Dublin', 0.164125], ['Paris', 0.165375], ['Steakhouse', 0.165625], ['Nepali', 0.16575], ['African', 0.1643125], ['Berlin', 0.1655], ['Pakistani', 0.1655625], ['Street Food', 0.16525], ['Diner', 0.164375], ['Copenhagen', 0.16525], ['Vietnamese', 0.165], ['Barbecue', 0.1656875], ['Hungarian', 0.1650625], ['Lisbon', 0.1643125], ['Scottish', 0.16375], ['Lebanese', 0.16425], ['Belgian', 0.1649375], ['Zurich', 0.165625], ['Austrian', 0.1656875], ['Eastern European', 0.16375], ['Geneva', 0.1639375], ['Brussels', 0.164625], ['Vienna', 0.1645], ['Luxembourg', 0.1649375], ['London', 0.1644375], ['Oslo', 0.1646875], ['Scandinavian', 0.1639375], ['South American', 0.164875], ['Swiss', 0.1655625], ['Brew Pub', 0.16425], ['Irish', 0.1649375], ['Taiwanese', 0.16425], ['Brazilian', 0.1653125], ['Ethiopian', 0.1638125], ['Balti', 0.1635625], ['Moroccan', 0.164875], ['Cajun & Creole', 0.164625], ['Swedish', 0.1653125], ['Latin', 0.16575], ['Ljubljana', 0.1648125], ['Colombian', 0.164625], ['Peruvian', 0.164125], ['Caribbean', 0.16575], ['Argentinean', 0.1648125], ['Central American', 0.165875], ['Sri Lankan', 0.165], ['Kosher', 0.164625], ['Cuban', 0.1640625], ['Russian', 0.1643125], ['Ukrainian', 0.16525], ['Tunisian', 0.166125], ['Croatian', 0.1645], ['Arabic', 0.164375], ['Afghani', 0.165], ['Norwegian', 0.1655625], ['Israeli', 0.16525], ['Georgian', 0.1635], ['Romanian', 0.1645], ['Armenian', 0.1658125], ['Slovenian', 0.16525], ['Venezuelan', 0.1641875], ['Tibetan', 0.1636875], ['Southwestern', 0.1645], ['unique_Cuisine_Style', 0.1644375], ['Central Asian', 0.1643125], ['Jamaican', 0.164875], ['Cambodian', 0.1645], ['Chilean', 0.165125], ['Filipino', 0.164375], ['Mongolian', 0.165625], ['Singaporean', 0.1643125], ['Egyptian', 0.1645625], ['Welsh', 0.165875], ['Fujian', 0.1645625], ['Minority Chinese', 0.1638125], ['Ecuadorean', 0.1646875], ['Uzbek', 0.1639375], ['Azerbaijani', 0.163875], ['Latvian', 0.1646875], ['New Zealand', 0.16475], ['Albanian', 0.16525], ['Caucasian', 0.163875], ['Canadian', 0.165625], ['Polynesian', 0.164], ['Salvadoran', 0.164125], ['Xinjiang', 0.164375], ['Burmese', 0.163375], ['Native American', 0.164875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = sorted(log_list, key=lambda x: x[1], reverse=False)\n",
    "print(*new_list, sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_list_remove_columns = []\n",
    "my.test_model(df[df['Sample']==1],fin_list_remove_columns, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['norm_Ranking_on_Popul_in_City'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Australian', 'one_Review', 'outliers_Number_of_Reviews', 'norm_Ranking_on_Popul_in_City', 'Korean', 'Japanese', 'Turkish', 'Malaysian', 'Indonesian', 'Hawaiian', 'code_Cuisine Style', 'norm_Population_on_Rest', 'Amsterdam', 'Hamburg', 'doble_Ranking', 'Warsaw', 'Persian', 'Soups', 'Mexican', 'Bangladeshi', 'Yunnan', 'American', 'empty_Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.query('Sample == 1').drop(['Sample']+drop_list, axis=1)\n",
    "test_data = df.query('Sample == 0').drop(['Sample','Rating']+drop_list, axis=1)\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../kaggle/data/sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.shape, test_data.shape, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission=my_vec_round(predict_submission)\n",
    "predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
