{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Перебор по количеству кластеров в разных алгоритмах кластеризации\n",
    "from collections import defaultdict\n",
    "silhouette_dict = defaultdict(list)\n",
    "# цикл по числу кластеров: от 2 до 10\n",
    "for n in range(2, 11):\n",
    "    # инициализируем алгоритмы:\n",
    "    KM = KMeans(n_clusters=n, random_state=42)\n",
    "    gm = GaussianMixture(n_components=n, random_state=42)\n",
    "    ac = AgglomerativeClustering(n_clusters=n)\n",
    "    # создаём словарь, где ключи - названия алгоритмов, значения - сами алгоритмы:\n",
    "    alg_dict = {'K-means': KM, 'EM-алгоритм': gm, 'Агломеративная кластеризация': ac}\n",
    "    # цикл по словарю:\n",
    "    for alg_name, algo in alg_dict.items():\n",
    "        labels = algo.fit_predict(X) # получаем предсказания\n",
    "        sil_score = silhouette_score(X, labels) # считаем коэффициент силуэта\n",
    "        # добавляем в словарь в list, соответствующему рассматриваемому алгоритму, \n",
    "        # пару вида : (число кластеров, коэффициент силуэта)\n",
    "        silhouette_dict[alg_name].append((n, sil_score)) \n",
    "# цикл по ключам словаря с коэффициентами силуэта для алгоритмов:\n",
    "for alg_name in silhouette_dict.keys():\n",
    "    # сохраняем число кластеров и коэф. силуэта для пары, \n",
    "    # в которой коэф. максимальный для данного алгоритма:\n",
    "    n_clusters, sil_score = max(silhouette_dict[alg_name], key=lambda x: x[1])\n",
    "    # выводим  название алгоритма и искомое число кластеров (и коэф. силуэта):\n",
    "    print(f\"{alg_name} : {n_clusters}, {sil_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#визуализация аглломеративного алгоритма\n",
    "df = pd.read_csv(\"./food.csv\", sep=' ')\n",
    "X = df.drop('Name', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X)\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(X_transformed, method='average', metric='euclidean')\n",
    "names = df.Name.values\n",
    "dend = dendrogram(Z, color_threshold=0, labels=names, \n",
    "                  orientation='left')\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "t = 2.3\n",
    "labels = fcluster(Z, t, criterion='distance')\n",
    "print(len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стекинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature(clf, X_train, X_test, y_train, cv):\n",
    "    \"\"\"\n",
    "    разбивает выборку на фолды и обчает указанный алгоритм на cv-1 фолде\n",
    "    \n",
    "    Computes meta-features using the classifier.\n",
    "    \n",
    "    :arg clf: scikit-learn classifier\n",
    "    :args X_train, y_train: training set\n",
    "    :arg X_test: testing set\n",
    "    :arg cv: cross-validation folding\n",
    "    \"\"\"\n",
    "    X_meta_train = np.zeros_like(y_train, dtype=np.float32)\n",
    "    for train_fold_index, predict_fold_index in cv.split(X_train):\n",
    "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]\n",
    "        \n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(X_fold_predict)[:, 1]\n",
    "    \n",
    "    meta_clf = clone(clf)\n",
    "    meta_clf.fit(X_train, y_train)\n",
    "    \n",
    "    X_meta_test = meta_clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return X_meta_train, X_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metafeatures(classifiers, X_train, X_test, y_train, cv):\n",
    "    \"\"\"\n",
    "    Generates metafeatures using a list of classifiers.\n",
    "    \n",
    "    :arg classifiers: list of scikit-learn classifiers\n",
    "    :args X_train, y_train: training set\n",
    "    :arg X_test: testing set\n",
    "    :arg cv: cross-validation folding\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        compute_meta_feature(clf, X_train, X_test, y_train, cv)\n",
    "        for clf in tqdm(classifiers)\n",
    "    ]\n",
    "    \n",
    "    stacked_features_train = np.vstack([\n",
    "        features_train for features_train, features_test in features\n",
    "    ]).T\n",
    "\n",
    "    stacked_features_test = np.vstack([\n",
    "        features_test for features_train, features_test in features\n",
    "    ]).T\n",
    "    \n",
    "    return stacked_features_train, stacked_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_feature_mean(clf, X_train, X_test, y_train, cv):\n",
    "    \"\"\"\n",
    "    Эта функция подсчитывает признаки для мета-классификатора. \n",
    "    Они являются вероятностями классов при решении задачи многоклассовой классификации.\n",
    "\n",
    "    :arg clf: классификатор\n",
    "    :args X_train, y_train: обучающая выборка\n",
    "    :arg X_test: признаки тестовой выборки\n",
    "    :arg cv: класс, генерирующий фолды (KFold)\n",
    "\n",
    "    :returns X_meta_train, X_meta_test: новые признаки для обучающей и тестовой выборок\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    X_meta_train = np.zeros((len(X_train), n_classes), dtype=np.float32)\n",
    "    X_meta_tests_array = []\n",
    "    splits = 0\n",
    "    for train_fold_index, predict_fold_index in cv.split(X_train):\n",
    "        n_classes = len(np.unique(y_test))\n",
    "        X_meta_test = np.zeros((len(X_test), n_classes), dtype=np.float32)\n",
    "        splits += 1\n",
    "        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n",
    "        y_fold_train = y_train[train_fold_index]\n",
    "        folded_clf = clone(clf)\n",
    "        folded_clf.fit(X_fold_train, y_fold_train)\n",
    "        X_meta_train[predict_fold_index] = folded_clf.predict_proba(X_fold_predict)\n",
    "        X_meta_tests_array.append(folded_clf.predict_proba(X_test))\n",
    "    meta_clf = clone(clf)\n",
    "    meta_clf.fit(X_train, y_train)\n",
    "    X_meta_test = sum(X_meta_tests_array) / splits\n",
    "    return X_meta_train, X_meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статистика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тест стьюдента\n",
    "#\n",
    "data_new=data.copy()\n",
    "def get_stat_dif(column):\n",
    "    cols = data[data['Sample']==1].loc[:, column].value_counts().index[:10]\n",
    "    combinations_all = list(combinations(cols, 2))\n",
    "    for comb in combinations_all:\n",
    "        if ttest_ind(data[data['Sample']==1].loc[data_new.loc[:, column] == comb[0], 'Rating'],\n",
    "                     data[data['Sample']==1].loc[data_new.loc[:, column] == comb[1], 'Rating']).pvalue \\\n",
    "                <= 0.05/len(combinations_all):  # Учли поправку Бонферони\n",
    "            print('Найдены статистически значимые различия для колонки', column)\n",
    "        else:\n",
    "            print('Не найдены статистически значимые различия для колонки', column)\n",
    "            break\n",
    "\n",
    "            \n",
    "for col in data_new.columns.tolist():\n",
    "     get_stat_dif(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_info(df):\n",
    "    print('Размерность датасета: ', df.shape)\n",
    "    display(df.head(5))\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_nan_values(d_df):\n",
    "    total = d_df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (d_df.isnull().sum()/d_df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    display(missing_data[missing_data['Total']>0])\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_without_plots_num_collumns(d_df,columns):\n",
    "    #numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    num_df = d_df[columns]\n",
    "    list_of_names = list(num_df.columns)\n",
    "    temp_dict = {}\n",
    "    temp_dict['имя признака'] = list_of_names\n",
    "    temp_dict['тип'] = num_df.dtypes\n",
    "    temp_dict['# пропусков(NaN)'] = num_df.isnull().sum().values \n",
    "    temp_dict['# уникальных'] = num_df.nunique().values\n",
    "    temp_dict['минимум'] = num_df.describe(include='all').loc['min']\n",
    "    temp_dict['среднее'] = num_df.describe(include='all').loc['mean']\n",
    "    temp_dict['медиана'] = num_df.describe(include='all').loc['50%']\n",
    "    temp_dict['макс'] = num_df.describe(include='all').loc['max']\n",
    "    temp_dict['std'] = pd.DataFrame(num_df.std().values,columns=['std']).loc[:,'std']\n",
    "    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n",
    "    display(temp_df.T)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_by_plots_num_collumns(d_df,columns,font_size=12):\n",
    "    fig, axes = plt.subplots(len(columns), 4, figsize=(8, 5*len(columns)))\n",
    "    for i in range(0,len(columns)):\n",
    "        axes[i,0].hist(d_df[columns[i]],label=str(columns[i]))\n",
    "        axes[i,0].set_title(str(columns[i]),fontsize=font_size)\n",
    "        axes[i,1].boxplot(d_df[columns[i]])\n",
    "        axes[i,1].set_title(str(columns[i]),fontsize=font_size)\n",
    "        axes[i,2].hist(np.log(d_df[columns[i]]+1),label=str(columns[i]))\n",
    "        axes[i,2].set_title('log_'+str(columns[i]),fontsize=font_size)\n",
    "        axes[i,3].boxplot(np.log(d_df[columns[i]]+1))\n",
    "        axes[i,3].set_title('log_'+str(columns[i]),fontsize=font_size)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def borders_of_outliers(d_df,columns, log = False):\n",
    "    num_df = d_df[columns]\n",
    "    list_of_names = list(num_df.columns)\n",
    "    temp_dict = {}\n",
    "    temp_dict['имя признака'] = list_of_names\n",
    "    left_board=[]\n",
    "    right_board=[]\n",
    "    left_outliers=[]\n",
    "    right_outliers=[]\n",
    "    path_left_outliers=[]\n",
    "    path_right_outliers=[]\n",
    "    if log:\n",
    "        log_left_board=[]\n",
    "        log_right_board=[]\n",
    "        log_left_outliers=[]\n",
    "        log_right_outliers=[]\n",
    "        log_path_left_outliers=[]\n",
    "        log_path_right_outliers=[]\n",
    "    for i in columns:\n",
    "        IQR = num_df[i].quantile(0.75) - num_df[i].quantile(0.25)\n",
    "        perc25 = num_df[i].quantile(0.25)\n",
    "        perc75 = num_df[i].quantile(0.75)\n",
    "        left_border = num_df[i].quantile(0.25) - 1.5*IQR\n",
    "        right_border = perc75 + 1.5*IQR\n",
    "        left_board.append(left_border)\n",
    "        right_board.append(right_border)\n",
    "        left_outliers.append((num_df[i]<left_border).sum())\n",
    "        right_outliers.append((num_df[i]>right_border).sum())\n",
    "        path_left_outliers.append((num_df[i]<left_border).sum()/len(num_df[i]))\n",
    "        path_right_outliers.append((num_df[i]>right_border).sum()/len(num_df[i]))\n",
    "        if log:\n",
    "            num_df[i]=num_df[i].apply(lambda x: math.log(x+1))\n",
    "            IQR = num_df[i].quantile(0.75) - num_df[i].quantile(0.25)\n",
    "            perc25 = num_df[i].quantile(0.25)\n",
    "            perc75 = num_df[i].quantile(0.75)\n",
    "            left_border = num_df[i].quantile(0.25) - 1.5*IQR\n",
    "            right_border = perc75 + 1.5*IQR\n",
    "            log_left_board.append(left_border)\n",
    "            log_right_board.append(right_border)\n",
    "            log_left_outliers.append((num_df[i]<left_border).sum())\n",
    "            log_right_outliers.append((num_df[i]>right_border).sum())\n",
    "            log_path_left_outliers.append((num_df[i]<left_border).sum()/len(num_df[i]))\n",
    "            log_path_right_outliers.append((num_df[i]>right_border).sum()/len(num_df[i]))\n",
    "        \n",
    "    temp_dict['левая граница'] = left_board\n",
    "    temp_dict['правая граница'] = right_board\n",
    "    temp_dict['выбросов слева'] = left_outliers\n",
    "    temp_dict['выбросов справа'] = right_outliers\n",
    "    temp_dict['доля выбросов слева'] = path_left_outliers\n",
    "    temp_dict['доля выбросов справа'] = path_right_outliers\n",
    "    \n",
    "    if log:\n",
    "        temp_dict['левая граница с логарифмом'] = log_left_board\n",
    "        temp_dict['правая граница с логарифмом'] = log_right_board\n",
    "        temp_dict['выбросов слева с логарифмом'] = log_left_outliers\n",
    "        temp_dict['выбросов справа с логарифмом'] = log_right_outliers\n",
    "        temp_dict['доля выбросов слева с логарифмом'] = log_path_left_outliers\n",
    "        temp_dict['доля выбросов справа с логарифмом'] = log_path_right_outliers\n",
    "    \n",
    "    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n",
    "    display(temp_df.T)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_describer(df,param,bins=20):\n",
    "    nulls=round(df[param].isnull().sum()/len(df[param]),4)\n",
    "    low = df[param].min()\n",
    "    median = df[param].median()\n",
    "    mean = df[param].mean()\n",
    "    hight = df[param].max()\n",
    "    IQR = df[param].quantile(\n",
    "        0.75) - df[param].quantile(0.25)\n",
    "    perc25 = df[param].quantile(0.25)\n",
    "    perc75 = df[param].quantile(0.75)\n",
    "    #blowout=len(df[(df[param]<perc25 - 1.5*IQR)|(df[param]>perc75 + 1.5*IQR)])\n",
    "    print('доля пропусков : {} \\n\\r'.format(nulls),\n",
    "          'min : {} \\n\\r'.format(low),\n",
    "          '25-й перцентиль: {} \\n\\r'.format(perc25),\n",
    "          'медиана: {} \\n\\r'.format(median),\n",
    "          'среднее: {} \\n\\r'.format(mean),\n",
    "          'max : {} \\n\\r'.format(hight),\n",
    "          '75-й перцентиль: {} \\n\\r'.format(perc75),\n",
    "          \"IQR: {} \\n\\r\".format(IQR),\n",
    "          \"Границы выбросов: [{f}, {l}] \\n\\r\".format(f=perc25 - 1.5*IQR, l=perc75 + 1.5*IQR),\n",
    "         )#'Количество выбросов: \\n\\r'.format(blowout))\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "    nrows=1, ncols=2,\n",
    "    figsize=(8, 4))\n",
    "    sns.distplot(df[param],ax=ax1)\n",
    "    #ax1.hist(df[param],label = param,bins=bins)\n",
    "    ax1.set_title('Распределение значений')\n",
    "    ax1.legend()\n",
    "    sns.distplot(np.log(df[param]+1),ax=ax2)\n",
    "    #ax2.hist(np.log(df[param]+1),label = param,bins=bins)\n",
    "    ax2.set_title('Логорифмированное Распределение значений')\n",
    "    ax2.legend()\n",
    "    #sns.distplot(df[param])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Категориальные и биномиальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_without_plots_cat_bin_collumns(d_df,columns):\n",
    "    cat_bin_df = d_df[columns]\n",
    "    list_of_names = list(cat_bin_df.columns)\n",
    "    temp_dict = {}\n",
    "    temp_dict['имя признака'] = list_of_names\n",
    "    temp_dict['тип'] = cat_bin_df.dtypes\n",
    "    temp_dict['# пропусков(NaN)'] = cat_bin_df.isnull().sum().values \n",
    "    temp_dict['# уникальных'] = cat_bin_df.nunique().values\n",
    "    temp_dict['# мода'] = cat_bin_df.mode()\n",
    "    temp_df = pd.DataFrame.from_dict(temp_dict, orient='index')\n",
    "    display(temp_df.T)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_param_describe(df,param):\n",
    "    print('доля пропусков: ',df[param].isnull().sum()/len(df[param]))\n",
    "    print('Значения параметра и их количество')\n",
    "    df[param].value_counts().plot.bar()\n",
    "    print(df[param].value_counts())\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.boxplot(x=param, y='default', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Рисуем результат работы регрессионной модели\n",
    "def Regression_plot(X,y,X_test,Y_test):\n",
    "    #X- все примеры\n",
    "    #У- все метки\n",
    "    #X_test - тестовая/обучающая выборка нашеймодели\n",
    "    #Y_test - результат работы модели\n",
    "    plt.figure()\n",
    "    #Отрисовываем наши точки\n",
    "    plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
    "                c=\"darkorange\", label=\"data\")\n",
    "    #Отрисовываем кривую модели\n",
    "    plt.plot(X_test, Y_test, color=\"yellowgreen\", label=\"Model\", linewidth=2)\n",
    "    plt.xlabel(\"data\")\n",
    "    plt.ylabel(\"target\")\n",
    "    plt.title(\"Decision Tree Regression\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "Regression_plot(X,y,X_test,y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Визуализация дерева без текста\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "tree.plot_tree(model, max_depth=5, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "#Показывает качество работы модели в зависимости от параметра\n",
    "\n",
    "def get_dependency_and_plot(parameters, param_title=None):\n",
    "    # parameters: tuple, где первый элемент - название параметра RandomForestClassifier, (например, n_estimators),\n",
    "    # а второй - диапазон для заданного параметра.\n",
    "    #\n",
    "    # param_title: Продолжение фразы 'Зависимость accuracy от ...' из заголовка графика.\n",
    "    # Опциональный параметр. Если None, то заголовок не оторбражается\n",
    "    param_name, param_range = parameters\n",
    "    accuracy_list = []\n",
    "    # цикл по диапазону параметра:\n",
    "    for value in param_range:\n",
    "        # словарь параметров для задания параметров модели через метод set_params()\n",
    "        param_dict = {param_name: value}\n",
    "        model = RandomForestClassifier()\n",
    "        model.set_params(**param_dict)\n",
    "        # считаем accuracy на кросс-валидации и берём среднее:\n",
    "        score = cross_val_score(model, X, Y, cv=10, n_jobs=-1).mean()\n",
    "        accuracy_list.append((value, score))\n",
    "    # отрисовываем график:\n",
    "    xx = [z[0] for z in accuracy_list]\n",
    "    yy = [z[1] for z in accuracy_list]\n",
    "    plt.plot(xx, yy, '.-')\n",
    "    if param_title != None:\n",
    "        plt.title('Зависимость accuracy от ' + param_title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean accuracy\")\n",
    "\n",
    "\n",
    "n_estimators_range = np.arange(1, 200, 10, dtype=int)\n",
    "max_features_range = np.arange(5, X.shape[1], 5, dtype=int)\n",
    "max_depth_range = np.arange(3, 30, 2, dtype=int)\n",
    "# Также определим пары tuple для работы с определённой выше функцией:\n",
    "est_tuple = ('n_estimators', n_estimators_range)\n",
    "max_features_tuple = ('max_features', max_features_range)\n",
    "max_depth_tuple = ('max_depth', max_depth_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
